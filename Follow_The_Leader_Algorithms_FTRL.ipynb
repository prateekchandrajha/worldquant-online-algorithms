{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Follow-The-Leader-Algorithms-FTRL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCSCIHmJwTExoeGI0cYts5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekchandrajha/worldquant-online-algorithms/blob/main/Follow_The_Leader_Algorithms_FTRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WutCcK2eMxj"
      },
      "source": [
        "import random\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "random.seed(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FQyzaPRffFP"
      },
      "source": [
        "References for FTRL Algorithm:\r\n",
        "\r\n",
        "- \"Follow the Regularized Leader\" method provides a framework to design and analyze online algorithms in a versatile and well-motivated way. We will be using FTRL to simulate trading before the final submission in February.\r\n",
        "\r\n",
        "- https://medium.com/@dhirajreddy13/factorization-machines-and-follow-the-regression-leader-for-dummies-7657652dce69\r\n",
        "\r\n",
        "- FTRL and Mirror Descent Algorithms: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37013.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNy7RJA8eY_U"
      },
      "source": [
        "class FollowTheRegularizedLeader(object):\r\n",
        "    def __init__(self, alpha, beta, l1, l2, features, reg_func, indices):\r\n",
        "        self.alpha = alpha\r\n",
        "        self.beta = beta\r\n",
        "        self.l1 = l1\r\n",
        "        self.l2 = l2\r\n",
        "        self.reg_func = reg_func\r\n",
        "        self.indices = [0]\r\n",
        "        self.count = 0\r\n",
        "        for index in indices:\r\n",
        "            self.indices.append(index)\r\n",
        "        self.sum_of_gradients_squared = [0.] * features\r\n",
        "        self.weights = {}\r\n",
        "        self.temp_weights = []\r\n",
        "        i = 0\r\n",
        "        while i < features:\r\n",
        "            self.temp_weights.append(random.random())\r\n",
        "            i += 1\r\n",
        "\r\n",
        "    def update(self, probability, result):\r\n",
        "        gradient = probability - result\r\n",
        "        gradient_squared = math.pow(gradient, 2)\r\n",
        "        self.count += 1\r\n",
        "        if self.reg_func is \"RDA\":\r\n",
        "            for i in self.indices:\r\n",
        "                sigma = (math.sqrt(self.sum_of_gradients_squared[i] + gradient_squared)) / (self.alpha * self.count)\r\n",
        "                self.temp_weights[i] += -(sigma * self.weights[i]) + gradient\r\n",
        "                self.sum_of_gradients_squared[i] += gradient_squared\r\n",
        "        elif self.reg_func is \"OPG\":\r\n",
        "            for i in self.indices:\r\n",
        "                sigma = (math.sqrt(self.sum_of_gradients_squared[i] + gradient_squared) - math.sqrt(self.sum_of_gradients_squared[i])) / self.alpha\r\n",
        "                self.temp_weights[i] += -(sigma * self.weights[i]) + gradient\r\n",
        "                self.sum_of_gradients_squared[i] += gradient_squared\r\n",
        "\r\n",
        "    def predict(self):\r\n",
        "        weights = {}\r\n",
        "        function = \"Sigmoid\"\r\n",
        "        w_inner_x = float(0)\r\n",
        "        for i in self.indices:\r\n",
        "            sign = float(np.sign(self.temp_weights))\r\n",
        "            if sign * self.temp_weights[i] <= self.l1:\r\n",
        "                weights[i] = float(0)\r\n",
        "            else:\r\n",
        "                weights[i] = (sign * self.l1 - self.temp_weights[i]) / ((self.beta + math.sqrt(self.sum_of_gradients_squared[i])) / self.alpha + self.l2)\r\n",
        "            w_inner_x += weights[i]\r\n",
        "\r\n",
        "        self.weights = weights\r\n",
        "\r\n",
        "        if function is \"Sigmoid\":\r\n",
        "            probability = float(1) / (float(1) + math.exp(-max(min(float(w_inner_x), float(100)), float(-100))))\r\n",
        "        elif function is \"ReLU\":\r\n",
        "            probability = max(float(0), max(min(float(w_inner_x), float(100)), float(-100)))\r\n",
        "\r\n",
        "        return probability\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def log_loss(true_label, predicted, eps=1e-15):\r\n",
        "        p = np.clip(predicted, eps, 1 - eps)\r\n",
        "        if true_label > 0:\r\n",
        "            return -math.log(p)\r\n",
        "        else:\r\n",
        "            return -math.log(1 - p)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3xp8UHef2Y"
      },
      "source": [
        "# Declaring FTRL Parameters\r\n",
        "\r\n",
        "alpha = 0.2\r\n",
        "beta = 0.5\r\n",
        "l1 = 0.32\r\n",
        "l2 = 0.32\r\n",
        "# features = number of features\r\n",
        "# reg_func = either \"RDA\" or \"OPG\"\r\n",
        "# indices\r\n",
        "\r\n",
        "FTRL_simulate = FollowTheRegularizedLeader(alpha, beta, l1, l2, features, reg_func, indices) # declaring an object of FTRL algorithm class for performing simulations"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}